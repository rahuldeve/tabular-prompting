{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This project is an experiment to see how one can world knowledge into classical ML models like Decision Trees. \n",
                "\n",
                "Deep Learning models are not competitive in tabular datasets. Models like XGBoost are on par with the best DL approaches have to offer but require considerably less training time and can be tuned easily. On the other hand, DL models can levarage large scale language models like GPT-2 easily to incorporate real world knowledge in their decision making.\n",
                "\n",
                "A typical way DL models incorporate pre-trained language models is by incorporating them as a submodule and training on the whole dataset. Such approaches cannot be easily extended onto classical ML models that dont use gradient descent as the way to optimize them.\n",
                "\n",
                "The \"knowledge\" from language models are usually contained in the embeddings they generate. Since these embeddings are just vectors, we can just use them as the inuts of classical ML models. So how do we get these embeddings? \n",
                "\n",
                "We could always just take a row from a table, write it down as a sentence and feed it to the language model. But the way we construct our sentence has a big impact on the embeddings generated by the language model. Language models are trained to predict words given a sentence. If we just naively craft a table row as a sentence, the model might not get the task that we are expecting it to do (eg. generating an embedding that can help a linear regressor better predict a certain value. Ideally this embedding will contain some external information that can be used to infer our target variable).\n",
                "\n",
                "The problem then becomes on how we can get language model to give us the \"right\" knowledge? This is where prompting language models come in. Prompting involves crafting our input in such a way that the it helps the language model get the larger context of what it expects us to do. Prompting has been succesfull in making large language models like GPT-3 perform zero-shot tasks on a variety of tasks that it was never trained for.\n",
                "\n",
                "But before we see how prompting can help in enchancing XGBoost on tabular data, let us first get a dataset for which we can easily incorporate some real world knowledge and build a baseline XGBoost model. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\rahul\\miniconda3\\envs\\tabular\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
                        "  from pandas import MultiIndex, Int64Index\n"
                    ]
                }
            ],
            "source": [
                "from copy import deepcopy\n",
                "import itertools\n",
                "import pickle\n",
                "from functools import partial\n",
                "\n",
                "import polars as pl\n",
                "import numpy as np\n",
                "\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
                "from sklearn.metrics import mean_squared_error\n",
                "\n",
                "import torch\n",
                "from torch.optim import Adam\n",
                "from torch.nn import CosineEmbeddingLoss\n",
                "from torch.utils.data import DataLoader\n",
                "\n",
                "from transformers import AutoTokenizer, AutoModel\n",
                "from sentence_transformers import InputExample, SentenceTransformer, util\n",
                "import xgboost\n",
                "\n",
                "from data_utils import *\n",
                "from model import *\n",
                "from prompt_utils import *\n",
                "\n",
                "from tqdm.notebook import tqdm\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1 \"class=\"dataframe \">\n",
                            "<thead>\n",
                            "<tr>\n",
                            "<th>\n",
                            "Year\n",
                            "</th>\n",
                            "<th>\n",
                            "Month\n",
                            "</th>\n",
                            "<th>\n",
                            "DayofMonth\n",
                            "</th>\n",
                            "<th>\n",
                            "DayOfWeek\n",
                            "</th>\n",
                            "<th>\n",
                            "Origin\n",
                            "</th>\n",
                            "<th>\n",
                            "counts\n",
                            "</th>\n",
                            "<th>\n",
                            "airport\n",
                            "</th>\n",
                            "</tr>\n",
                            "<tr>\n",
                            "<td>\n",
                            "i64\n",
                            "</td>\n",
                            "<td>\n",
                            "i64\n",
                            "</td>\n",
                            "<td>\n",
                            "i64\n",
                            "</td>\n",
                            "<td>\n",
                            "i64\n",
                            "</td>\n",
                            "<td>\n",
                            "str\n",
                            "</td>\n",
                            "<td>\n",
                            "u32\n",
                            "</td>\n",
                            "<td>\n",
                            "str\n",
                            "</td>\n",
                            "</tr>\n",
                            "</thead>\n",
                            "<tbody>\n",
                            "<tr>\n",
                            "<td>\n",
                            "2001\n",
                            "</td>\n",
                            "<td>\n",
                            "1\n",
                            "</td>\n",
                            "<td>\n",
                            "9\n",
                            "</td>\n",
                            "<td>\n",
                            "2\n",
                            "</td>\n",
                            "<td>\n",
                            "\"SBN\"\n",
                            "</td>\n",
                            "<td>\n",
                            "6\n",
                            "</td>\n",
                            "<td>\n",
                            "\"South Bend Regional\"\n",
                            "</td>\n",
                            "</tr>\n",
                            "<tr>\n",
                            "<td>\n",
                            "2001\n",
                            "</td>\n",
                            "<td>\n",
                            "3\n",
                            "</td>\n",
                            "<td>\n",
                            "27\n",
                            "</td>\n",
                            "<td>\n",
                            "2\n",
                            "</td>\n",
                            "<td>\n",
                            "\"GSP\"\n",
                            "</td>\n",
                            "<td>\n",
                            "21\n",
                            "</td>\n",
                            "<td>\n",
                            "\"Greenville-Spartanburg\"\n",
                            "</td>\n",
                            "</tr>\n",
                            "<tr>\n",
                            "<td>\n",
                            "2001\n",
                            "</td>\n",
                            "<td>\n",
                            "3\n",
                            "</td>\n",
                            "<td>\n",
                            "29\n",
                            "</td>\n",
                            "<td>\n",
                            "4\n",
                            "</td>\n",
                            "<td>\n",
                            "\"MIA\"\n",
                            "</td>\n",
                            "<td>\n",
                            "229\n",
                            "</td>\n",
                            "<td>\n",
                            "\"Miami International\"\n",
                            "</td>\n",
                            "</tr>\n",
                            "<tr>\n",
                            "<td>\n",
                            "2001\n",
                            "</td>\n",
                            "<td>\n",
                            "4\n",
                            "</td>\n",
                            "<td>\n",
                            "18\n",
                            "</td>\n",
                            "<td>\n",
                            "3\n",
                            "</td>\n",
                            "<td>\n",
                            "\"SJU\"\n",
                            "</td>\n",
                            "<td>\n",
                            "77\n",
                            "</td>\n",
                            "<td>\n",
                            "\"Luis Munoz Marin International\"\n",
                            "</td>\n",
                            "</tr>\n",
                            "<tr>\n",
                            "<td>\n",
                            "2001\n",
                            "</td>\n",
                            "<td>\n",
                            "4\n",
                            "</td>\n",
                            "<td>\n",
                            "8\n",
                            "</td>\n",
                            "<td>\n",
                            "7\n",
                            "</td>\n",
                            "<td>\n",
                            "\"GEG\"\n",
                            "</td>\n",
                            "<td>\n",
                            "31\n",
                            "</td>\n",
                            "<td>\n",
                            "\"Spokane Intl\"\n",
                            "</td>\n",
                            "</tr>\n",
                            "</tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "shape: (5, 7)\n",
                            "┌──────┬───────┬────────────┬───────────┬────────┬────────┬────────────────────────────────┐\n",
                            "│ Year ┆ Month ┆ DayofMonth ┆ DayOfWeek ┆ Origin ┆ counts ┆ airport                        │\n",
                            "│ ---  ┆ ---   ┆ ---        ┆ ---       ┆ ---    ┆ ---    ┆ ---                            │\n",
                            "│ i64  ┆ i64   ┆ i64        ┆ i64       ┆ str    ┆ u32    ┆ str                            │\n",
                            "╞══════╪═══════╪════════════╪═══════════╪════════╪════════╪════════════════════════════════╡\n",
                            "│ 2001 ┆ 1     ┆ 9          ┆ 2         ┆ SBN    ┆ 6      ┆ South Bend Regional            │\n",
                            "├╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
                            "│ 2001 ┆ 3     ┆ 27         ┆ 2         ┆ GSP    ┆ 21     ┆ Greenville-Spartanburg         │\n",
                            "├╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
                            "│ 2001 ┆ 3     ┆ 29         ┆ 4         ┆ MIA    ┆ 229    ┆ Miami International            │\n",
                            "├╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
                            "│ 2001 ┆ 4     ┆ 18         ┆ 3         ┆ SJU    ┆ 77     ┆ Luis Munoz Marin International │\n",
                            "├╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
                            "│ 2001 ┆ 4     ┆ 8          ┆ 7         ┆ GEG    ┆ 31     ┆ Spokane Intl                   │\n",
                            "└──────┴───────┴────────────┴───────────┴────────┴────────┴────────────────────────────────┘"
                        ]
                    },
                    "execution_count": 2,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "df = load_data(2000, 2002)\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The dataset above contains the number of flights from a specific airport in the United States during a particular date. The number of flights is given by the counts column. Our task is to predict this number, given the date and airport.\n",
                "\n",
                "Origin represents the unique iata indentifier for the airport in question. The actual name of the airport is given in the 'airport' column\n",
                "\n",
                "Right away we can see how external knowledge can help us here. Take an airport like Chicago O'Hare International. It is known to be pretty busy with an average of around 1000 flights coming and going out of the airport per day. Compare that to another airport like Dawson Community Airport, one of the quietest airports in the United States, we can easily guess the average number of flights per day.\n",
                "\n",
                "Let's take an XGBoost Regressor as the baseline and see how well it fares on this dataset. As usual with any ML task, we need to first split the dataset and prep it for the XGBoost model. The following code does this"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Split the dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\rahul\\miniconda3\\envs\\tabular\\lib\\site-packages\\polars\\internals\\frame.py:1483: UserWarning: accessing series as Attribute of a DataFrame is deprecated\n",
                        "  warnings.warn(\"accessing series as Attribute of a DataFrame is deprecated\")\n"
                    ]
                }
            ],
            "source": [
                "splits = train_test_split(df, test_size=0.2)\n",
                "df_train, df_test = splits\n",
                "\n",
                "# Casting for pylance\n",
                "df_train = cast(pl.DataFrame, df_train)\n",
                "df_test = cast(pl.DataFrame, df_test)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Featurize the train and test sets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Need to convert each airport id from a string to an integer. Using the Label encoder from scikit-learn for this purpose\n",
                "# The label encoder is fit for the whole dataset to prevent OOV when trying to transform the test set\n",
                "all_origins = df.select('Origin').distinct().Origin.to_numpy()\n",
                "origin_encoder = LabelEncoder()\n",
                "origin_encoder.fit(all_origins)\n",
                "\n",
                "# Applying the fitted label encoder to get the featurize the train set\n",
                "X_train = df_train.with_column(\n",
                "    pl.Series('origin_encoded', origin_encoder.transform(df_train.Origin.to_numpy()))\n",
                ").select([\n",
                "    pl.all().exclude(['Year', 'Origin', 'counts', 'airport', 'Month_name'])\n",
                "]).to_numpy()\n",
                "\n",
                "y_train = df_train.counts.to_numpy()\n",
                "\n",
                "\n",
                "# Applying the fitted label encoder to get the featurize the test set\n",
                "X_test = df_test.with_column(\n",
                "    pl.Series('origin_encoded', origin_encoder.transform(df_test.Origin.to_numpy()))\n",
                ").select([\n",
                "    pl.all().exclude(['Year', 'Origin', 'counts', 'airport', 'Month_name'])\n",
                "]).to_numpy()\n",
                "\n",
                "y_test = df_test.counts.to_numpy()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Train and evaluate the performance of the model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(3.2043302603855772, 34.39642720417406, 0.9983257424215061)"
                        ]
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "xgb = xgboost.XGBRegressor(\n",
                "    objective='reg:squarederror',\n",
                "    n_jobs = -1,\n",
                ")\n",
                "\n",
                "model = xgb.fit(X_train, y_train)\n",
                "y_pred = model.predict(X_test)\n",
                "\n",
                "r2 = r2_score(y_test, y_pred)\n",
                "mse = mean_squared_error(y_test, y_pred)\n",
                "mae = mean_absolute_error(y_test, y_pred)\n",
                "\n",
                "mae, mse, r2"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "def prompt_embeddings(df, prompt_model):\n",
                "    origins = df.select(['Origin', 'airport']).distinct()\n",
                "\n",
                "    embeddings = []\n",
                "    for airport in origins.airport:\n",
                "        query = prompt_model.encode(f'How crowded is the {airport} ?')\n",
                "        responses = prompt_model.encode([\n",
                "            f'{airport} is very crowded. There are more than 800 flights every day',\n",
                "            f'{airport} is moderately crowded. There around 400 flights every day',\n",
                "            f'{airport} is slightly crowded. There are around 100 flights every day',\n",
                "            f'{airport} is not crowded. There are less than 50 flights every day'\n",
                "        ])\n",
                "\n",
                "        sims = util.cos_sim(query, responses)\n",
                "        most_sim_idx = np.argmax(sims)\n",
                "        embeddings.append(responses[most_sim_idx])\n",
                "\n",
                "    embeddings = np.vstack(embeddings)\n",
                "\n",
                "    origins = origins.with_column(pl.Series('prompt_embeddings', embeddings))\n",
                "    origin_embedding_map = {k:v for k,v in origins.select(['Origin', 'prompt_embeddings']).rows()}\n",
                "\n",
                "    embeddings = df.select(\n",
                "        pl.col('Origin').apply(lambda x: origin_embedding_map[x]).alias('embeddings')\n",
                "    )\n",
                "    embeddings = embeddings['embeddings'].to_list()\n",
                "    embeddings = np.vstack(embeddings)\n",
                "    return embeddings"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "e45dd909a2fb4b979c33506e34e9036a",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Downloading:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "cee80aae289a48ee8c5ea0a804bc163e",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Downloading:   0%|          | 0.00/190 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "d20ec0bc75864b15bb2c81120cbb1feb",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Downloading:   0%|          | 0.00/10.2k [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "c961daebd15644928ecb24ce49745b95",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Downloading:   0%|          | 0.00/612 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "0895515ee19b47c7bba43f2579c4f8f8",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Downloading:   0%|          | 0.00/116 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "fa4b03a1840946ffb52873fd17664c2f",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Downloading:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "ed2f5b3efa2246af9db9cc806460c507",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Downloading:   0%|          | 0.00/349 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "c839a24480344993a88c58f785d5375a",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Downloading:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "d604832043054cc197aa63e7151997ab",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Downloading:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "9adf317d45844f10aac77ba833dca5f7",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "2038dda7a2a240aba2e8231806e4de88",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "1e7925c2d0e4451a9bf9ae623403e8ba",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Downloading:   0%|          | 0.00/350 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "a054c1d686094194b64691fa80ce8e90",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Downloading:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "dce39a9314f64b7ea4823a0b2481c084",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\rahul\\miniconda3\\envs\\tabular\\lib\\site-packages\\polars\\internals\\frame.py:1483: UserWarning: accessing series as Attribute of a DataFrame is deprecated\n",
                        "  warnings.warn(\"accessing series as Attribute of a DataFrame is deprecated\")\n"
                    ]
                }
            ],
            "source": [
                "prompt_model = SentenceTransformer('all-MiniLM-L6-v2', device='cuda', cache_folder='model_cache')\n",
                "X_embs_train = prompt_embeddings(df_train, prompt_model)\n",
                "X_embs_test = prompt_embeddings(df_test, prompt_model)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_train = df_train.with_column(\n",
                "    pl.Series('origin_encoded', origin_encoder.transform(df_train.Origin.to_numpy()))\n",
                ").select([\n",
                "    pl.all().exclude(['Year', 'Origin', 'counts', 'airport', 'Month_name'])\n",
                "]).to_numpy()\n",
                "\n",
                "X_train = np.hstack([X_train, X_embs_train])\n",
                "y_train = df_train.counts.to_numpy()\n",
                "\n",
                "\n",
                "X_test = df_test.with_column(\n",
                "    pl.Series('origin_encoded', origin_encoder.transform(df_test.Origin.to_numpy()))\n",
                ").select([\n",
                "    pl.all().exclude(['Year', 'Origin', 'counts', 'airport', 'Month_name'])\n",
                "]).to_numpy()\n",
                "\n",
                "X_test = np.hstack([X_test, X_embs_test])\n",
                "y_test = df_test.counts.to_numpy()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(1.6059000998000235, 15.93969158379574, 0.9992241301901905)"
                        ]
                    },
                    "execution_count": 9,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "xgb = xgboost.XGBRegressor(\n",
                "    objective='reg:squarederror',\n",
                "    n_jobs=-1)\n",
                "\n",
                "model = xgb.fit(X_train, y_train)\n",
                "y_pred = model.predict(X_test)\n",
                "\n",
                "r2 = r2_score(y_test, y_pred)\n",
                "mse = mean_squared_error(y_test, y_pred)\n",
                "mae = mean_absolute_error(y_test, y_pred)\n",
                "\n",
                "mae, mse, r2"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "num_prompts = 8\n",
                "\n",
                "query_prompt_format = pl.format(\n",
                "    'Airport: {}, Number of flights: {}', \n",
                "    pl.col('airport'), pl.col('airport_tokens')\n",
                ")\n",
                "\n",
                "value_prompt_format = pl.format(\n",
                "    'Airport: {}, Number of flights: {}', \n",
                "    pl.col('airport'), pl.col('counts')\n",
                ")\n",
                "\n",
                "query_value_prompts_train = generate_query_value_prompts(df_train, query_prompt_format, value_prompt_format, num_prompts)\n",
                "query_value_prompts_test = generate_query_value_prompts(df_test, query_prompt_format, value_prompt_format, num_prompts)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "airport_tokens = df.select('Origin').distinct()\n",
                "airport_tokens = airport_tokens['Origin']\n",
                "airport_tokens = airport_tokens.apply(partial(airport_token_sequencer, num_prompts=num_prompts)).to_list()\n",
                "airport_tokens = list(itertools.chain.from_iterable(airport_tokens))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "tokenizer = AutoTokenizer.from_pretrained('nreimers/MiniLM-L6-H384-uncased')\n",
                "prompt_model = AutoModel.from_pretrained('nreimers/MiniLM-L6-H384-uncased')\n",
                "\n",
                "num_added_tokens = tokenizer.add_tokens(airport_tokens, special_tokens=True)\n",
                "assert num_added_tokens == len(airport_tokens)\n",
                "\n",
                "pretrained_word_embeddings = deepcopy(prompt_model.embeddings.word_embeddings)\n",
                "prompt_model.resize_token_embeddings(len(tokenizer))\n",
                "\n",
                "# freeze all layers\n",
                "for param in prompt_model.parameters():\n",
                "    param.requires_grad = False\n",
                "\n",
                "prefrozen_word_embeddings = PrefrozenEmbeddings(pretrained_word_embeddings, num_added_tokens)\n",
                "prompt_model.embeddings.word_embeddings = prefrozen_word_embeddings"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": [
                "def convert_to_input_example_dataset(df) -> List[InputExample]:\n",
                "    return [\n",
                "        InputExample(texts=[q, v], label=1)\n",
                "        for q, v in zip(df.to_dict()['query'], df.to_dict()['value'])\n",
                "    ]\n",
                "\n",
                "\n",
                "prompt_train_set = convert_to_input_example_dataset(query_value_prompts_train.distinct())\n",
                "prompt_test_set = convert_to_input_example_dataset(query_value_prompts_test.distinct())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_dataloader = DataLoader(prompt_train_set, shuffle=True, batch_size=64, collate_fn=lambda x: x)      # type: ignore\n",
                "test_dataloader = DataLoader(prompt_test_set, shuffle=True, batch_size=64, collate_fn=lambda x: x)        # type: ignore"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_step(model, optimizer, criterion, tokenizer, device):\n",
                "    epoch_train_loss = 0.0\n",
                "    model.train()\n",
                "    for batch in train_dataloader:\n",
                "            query_embeddings = embed_sentences([i.texts[0] for i in batch], model, tokenizer, device)\n",
                "            value_embeddings = embed_sentences([i.texts[1] for i in batch], model, tokenizer, device)\n",
                "            labels = torch.tensor([i.label for i in batch]).to(device)\n",
                "            loss = criterion(query_embeddings, value_embeddings, labels)\n",
                "\n",
                "            loss.backward()\n",
                "            optimizer.step()\n",
                "            optimizer.zero_grad()\n",
                "            epoch_train_loss += loss.item()\n",
                "    \n",
                "    return epoch_train_loss\n",
                "\n",
                "\n",
                "def valid_step(model, criterion, tokenizer, device):\n",
                "    epoch_valid_loss = 0.0\n",
                "    model.eval()\n",
                "    with torch.no_grad():\n",
                "            for batch in test_dataloader:\n",
                "\n",
                "                query_embeddings = embed_sentences([i.texts[0] for i in batch], model, tokenizer, device)\n",
                "                value_embeddings = embed_sentences([i.texts[1] for i in batch], model, tokenizer, device)\n",
                "                labels = torch.tensor([i.label for i in batch]).to(device)\n",
                "                loss = criterion(query_embeddings, value_embeddings, labels)\n",
                "\n",
                "                epoch_valid_loss += loss.item()\n",
                "\n",
                "    return epoch_valid_loss"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
                "optimizer = Adam(prompt_model.parameters(), lr=5e-1)\n",
                "criterion = CosineEmbeddingLoss()\n",
                "prompt_model = prompt_model.to(device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "ac046517ee694c6695b9cec6b5fca06b",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%|          | 0/150 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "0 0.0631777877608935 0.028248991817235947\n",
                        "10 0.03607655167579651 0.011745750474242063\n",
                        "20 0.03319415425260862 0.009674091895039264\n",
                        "30 0.03185046911239624 0.008430726109788967\n",
                        "40 0.030811092754205068 0.007791874178040486\n",
                        "50 0.030440221726894378 0.007266210571217995\n",
                        "60 0.030016236876447996 0.007052317082595367\n",
                        "70 0.030018943175673485 0.006639283831016376\n",
                        "80 0.029090561096866925 0.006533061010906329\n",
                        "90 0.02915951795876026 0.006316139183651943\n",
                        "100 0.029200083017349242 0.006138484423550276\n",
                        "110 0.02873743958771229 0.006010062204530606\n",
                        "120 0.028976715728640558 0.005955656023266224\n",
                        "130 0.028235080341498058 0.00588332790021713\n",
                        "140 0.028053473805387814 0.005761411041021347\n",
                        "149 0.02845854461193085 0.005648391034740668\n"
                    ]
                }
            ],
            "source": [
                "def train(model_gpu, optimizer, criterion, device):\n",
                "    train_losses = []\n",
                "    valid_losses = []\n",
                "\n",
                "    num_epocs = 150\n",
                "    for e in tqdm(range(num_epocs)):\n",
                "\n",
                "        epoch_train_loss = train_step(model_gpu, optimizer, criterion, tokenizer, device)\n",
                "        epoch_valid_loss = valid_step(model_gpu, criterion, tokenizer, device)\n",
                "\n",
                "        train_losses.append(epoch_train_loss / len(train_dataloader))\n",
                "        valid_losses.append(epoch_valid_loss / len(test_dataloader))\n",
                "        \n",
                "        if e%10 == 0 or e == (num_epocs-1) or e == 0:\n",
                "            print(e, train_losses[-1], valid_losses[-1]) \n",
                "\n",
                "train(prompt_model, optimizer, criterion, device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": [
                "# torch.save(model, 'chk_1.pt')\n",
                "\n",
                "# with open('tok_1.pk', 'wb') as outfile:\n",
                "#     pickle.dump(tokenizer, outfile)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [],
            "source": [
                "# model = torch.load('chk_1.pt')\n",
                "# with open('tok_1.pk', 'rb') as infile:\n",
                "#     tokenizer = pickle.load(infile)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_prompt_embeddings(df, model, tokenizer, device):\n",
                "    query_prompts_train = pl.concat([\n",
                "        df.select('Origin'),\n",
                "        generate_query_value_prompts(\n",
                "            df, query_prompt_format, value_prompt_format, num_prompts\n",
                "        ).select('query')\n",
                "    ], how='horizontal').distinct()\n",
                "\n",
                "    with torch.no_grad():\n",
                "        prompt_embs_train = embed_sentences(\n",
                "            query_prompts_train.query.to_list(), \n",
                "            model, tokenizer, device\n",
                "        ).cpu()\n",
                "\n",
                "    origin_prompt_emb_map = dict()\n",
                "    for i in range(query_prompts_train.shape[0]):\n",
                "        origin = query_prompts_train['Origin'][i]\n",
                "        emb = prompt_embs_train[i, :].numpy()\n",
                "\n",
                "        origin_prompt_emb_map[origin] = emb\n",
                "\n",
                "    return np.vstack(\n",
                "        df.select([\n",
                "            pl.col('Origin').apply(lambda x: origin_prompt_emb_map[x]).alias('emb')\n",
                "        ]).emb.to_list()\n",
                "    )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\rahul\\miniconda3\\envs\\tabular\\lib\\site-packages\\polars\\internals\\frame.py:1483: UserWarning: accessing series as Attribute of a DataFrame is deprecated\n",
                        "  warnings.warn(\"accessing series as Attribute of a DataFrame is deprecated\")\n"
                    ]
                }
            ],
            "source": [
                "X_embs_train = generate_prompt_embeddings(df_train, prompt_model, tokenizer, device)\n",
                "X_embs_test = generate_prompt_embeddings(df_test, prompt_model, tokenizer, device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_train = df_train.with_column(\n",
                "    pl.Series('origin_encoded', origin_encoder.transform(df_train.Origin.to_numpy()))\n",
                ").select([\n",
                "    pl.all().exclude(['Year', 'Origin', 'counts', 'airport', 'Month_name'])\n",
                "]).to_numpy()\n",
                "\n",
                "X_train = np.hstack([X_train, X_embs_train])\n",
                "y_train = df_train.counts.to_numpy()\n",
                "\n",
                "\n",
                "X_test = df_test.with_column(\n",
                "    pl.Series('origin_encoded', origin_encoder.transform(df_test.Origin.to_numpy()))\n",
                ").select([\n",
                "    pl.all().exclude(['Year', 'Origin', 'counts', 'airport', 'Month_name'])\n",
                "]).to_numpy()\n",
                "\n",
                "X_test = np.hstack([X_test, X_embs_test])\n",
                "y_test = df_test.counts.to_numpy()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(1.4682715518835787, 13.406071752563278, 0.9993474549814046)"
                        ]
                    },
                    "execution_count": 23,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "xgb = xgboost.XGBRegressor(\n",
                "    objective='reg:squarederror', \n",
                "    n_jobs=-1)\n",
                "\n",
                "model = xgb.fit(X_train, y_train)\n",
                "y_pred = model.predict(X_test)\n",
                "\n",
                "r2 = r2_score(y_test, y_pred)\n",
                "mse = mean_squared_error(y_test, y_pred)\n",
                "mae = mean_absolute_error(y_test, y_pred)\n",
                "\n",
                "mae, mse, r2"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---"
            ]
        }
    ],
    "metadata": {
        "interpreter": {
            "hash": "976d43950396151e7cab9926e73b5abe20542c9ce0b57c070f05867d690adfc3"
        },
        "kernelspec": {
            "display_name": "Python 3.8.13 ('tabular')",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.13"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
